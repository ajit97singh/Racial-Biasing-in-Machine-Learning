{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:#ffffff;background-color:#181a1b\">\n",
    "<h2 style=\"color:#ffffff;background-color:#181a1b\">Racial bias in machine learning systems</h2> <a class=\"anchor\" id=\"partB\"></a> \n",
    "    \n",
    "![](https://static.propublica.org/projects/algorithmic-bias/assets/img/generated/opener-b-crop-2400*1350-00796e.jpg)\n",
    "\n",
    "<br />\n",
    "<b> Dataset Description </b><br />\n",
    "    \n",
    "The main dataset is the `compas.csv` <br />\n",
    "ProPublica's analysis is publically available [here](https://github.com/propublica/compas-analysis).\n",
    "\n",
    "The dataset was made publically available by **Northpointe**, an American tech-company that works with law establishment across several states in the US to predict future crimes based on past records of criminals.\n",
    "\n",
    "It has been suspected that the software used by Northpointe, `COMPAS`, is biased against the african american criminals, who end up with `high-risk` tags, despite minor criminal record, whereas `Caucasians` regularly received low-scores despite more significant criminal charges.\n",
    "    \n",
    "After pressure from several news agencies and a public investigation by ProPublica, the company released this dataset with a slice of the factors usually considered in order to assign a score to criminals.\n",
    "<br /><br />\n",
    "The dataset also contains a column `two_year_recid` with a binary response, i.e `1` if the released criminal ended up committing another crime within two years and `0` if the criminal did not commit a crime within a period of two years.\n",
    "    \n",
    "To learn more about this dataset, and the public investigation, you are highly recommended to read ProPublica's article on [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "sns.set()\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into 80% training and 20% validation sets stratified by race.\n",
    "\n",
    "Stratified mean that the two sets should have roughly similar distribution of races as the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data set \n",
    "df = pd.read_csv('data/compas.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data set between X(Predictor) and y(Response) variable\n",
    "\n",
    "X = df.drop(columns=['score_text','two_year_recid', 'c_charge_desc'])\n",
    "y = df[['score_text','two_year_recid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorical data\n",
    "\n",
    "X_encoding = {'race': {'Caucasian': 0, 'African-American': 1, \"Other\": 2, \"Hispanic\": 3, \"Native American\": 4, \"Asian\" : 5},\n",
    "              'c_charge_degree' : {\"M\": 0, \"F\": 1}}\n",
    "X_decoding = {'race': {0: 'Caucasian', 1: 'African-American', 2: \"Other\", 3: \"Hispanic\", 4: \"Native American\", 5 : \"Asian\"},\n",
    "              'c_charge_degree' : {0: \"M\", 1: \"F\"}}\n",
    "y_encoding = {'score_text': {\"Low\": 0, \"Medium\": 1, \"High\": 2}}\n",
    "y_decoding = {'score_text':{0: \"Low\", 1: \"Medium\", 2: \"High\"}}\n",
    "\n",
    "\n",
    "X = X.replace(X_encoding)\n",
    "y = y.replace(y_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=X['race'], random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subset of the dataframe containing only the relevent columns\n",
    "\n",
    "aa_cc = X[['race', 'age', 'priors_count', 'sex', 'length_of_stay_thresh']]\n",
    "\n",
    "# Dataframes containing data for the relevent races only\n",
    "aa = aa_cc[aa_cc['race'] == 1]\n",
    "cc = aa_cc[aa_cc['race'] == 0]\n",
    "\n",
    "# Plotting the desired graphs\n",
    "\n",
    "fig, axis = plt.subplots(2, 2, figsize=(18,12))\n",
    "\n",
    "axis[0,0].hist(aa['age'], alpha= 0.5)\n",
    "axis[0,0].hist(cc['age'], alpha= 0.5)\n",
    "axis[0,0].set_title(\"Age distribution for African-Americans and Caucasians\")\n",
    "axis[0,0].set_xlabel(\"Age\")\n",
    "axis[0,0].set_ylabel(\"Number of people\")\n",
    "axis[0,0].legend(['African-Americans', 'Caucasians'])\n",
    "\n",
    "axis[0,1].hist(aa['priors_count'], alpha= 0.5)\n",
    "axis[0,1].hist(cc['priors_count'], alpha= 0.5)\n",
    "axis[0,1].set_title(\"Prior Counts distribution for African-Americans and Caucasians\")\n",
    "axis[0,1].set_xlabel(\"Priors Counts\")\n",
    "axis[0,1].set_ylabel(\"Number of people\")\n",
    "axis[0,1].legend(['African-Americans', 'Caucasians'])\n",
    "\n",
    "axis[1,0].hist(aa['sex'].map({0:'Female', 1:'male'}), alpha= 0.5)\n",
    "axis[1,0].hist(cc['sex'].map({0:'Female', 1:'male'}), alpha= 0.5)\n",
    "axis[1,0].set_title(\"Sex distribution for African-Americans and Caucasians\")\n",
    "axis[1,0].set_xlabel(\"sex\")\n",
    "axis[1,0].set_ylabel(\"Number of people\")\n",
    "axis[1,0].legend(['African-Americans', 'Caucasians'])\n",
    "\n",
    "axis[1,1].hist(aa['length_of_stay_thresh'], alpha= 0.5)\n",
    "axis[1,1].hist(cc['length_of_stay_thresh'], alpha= 0.5)\n",
    "axis[1,1].set_title(\"Stay distribution for African-Americans and Caucasians\")\n",
    "axis[1,1].set_xlabel(\"Length of Stay\")\n",
    "axis[1,1].set_ylabel(\"Number of people\")\n",
    "axis[1,1].legend(['African-Americans', 'Caucasians'])\n",
    "\n",
    "\n",
    "plt.suptitle('Distribution of African-American and Caucasian population over different parameters',fontsize=20)\n",
    "\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> \n",
    "Building a logistic regression model to predict recidivism (two_year_recid) on these data and be sure to include race as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the ogistic refgression object\n",
    "\n",
    "lreg = LogisticRegression(random_state=66, max_iter=20000)\n",
    "\n",
    "# Redefining y for this question\n",
    "y_new_train = y_train['two_year_recid']\n",
    "y_new_val   = y_val['two_year_recid']\n",
    "\n",
    "# Fitting the model\n",
    "\n",
    "lreg.fit(X_train, y_new_train)\n",
    "\n",
    "# Making prediction \n",
    "\n",
    "y_train_pred = lreg.predict(X_train)\n",
    "y_val_pred   = lreg.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of the model\n",
    "\n",
    "accuracy = accuracy_score(y_new_val, y_val_pred)\n",
    "print(f\"The overall accuracy of the model is : {accuracy * 100:.4f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report further findings we will seperate the data by race \n",
    "\n",
    "# African-American data\n",
    "y_val_AA = y_new_val[X_val['race'] == 1]\n",
    "y_val_pred_AA = y_val_pred[X_val['race'] == 1]\n",
    "\n",
    "# Caucasian data\n",
    "\n",
    "y_val_CC = y_new_val[X_val['race'] == 0]\n",
    "y_val_pred_CC = y_val_pred[X_val['race'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating True Negative False Negative True Positive and False Positive\n",
    "\n",
    "# For African-Americans\n",
    "AA_confusion_matrix = confusion_matrix(y_val_AA, y_val_pred_AA)\n",
    "\n",
    "TN_AA = AA_confusion_matrix[0][0]\n",
    "FN_AA = AA_confusion_matrix[1][0]\n",
    "TP_AA = AA_confusion_matrix[1][1]\n",
    "FP_AA = AA_confusion_matrix[0][1]\n",
    "\n",
    "# True Positive Rate\n",
    "TPR_AA = TP_AA/(TP_AA+FN_AA)\n",
    "# True Negative Rate\n",
    "TNR_AA = TN_AA/(TN_AA+FP_AA) \n",
    "# False Positive Rate\n",
    "FPR_AA = FP_AA/(FP_AA+TN_AA)\n",
    "# False Negative Rate\n",
    "FNR_AA = FN_AA/(TP_AA+FN_AA)\n",
    "\n",
    "\n",
    "# For Caucasians\n",
    "CC_confusion_matrix = confusion_matrix(y_val_CC, y_val_pred_CC)\n",
    "\n",
    "TN_CC = CC_confusion_matrix[0][0]\n",
    "FN_CC = CC_confusion_matrix[1][0]\n",
    "TP_CC = CC_confusion_matrix[1][1]\n",
    "FP_CC = CC_confusion_matrix[0][1]\n",
    "\n",
    "# True Positive Rate\n",
    "TPR_CC = TP_CC/(TP_CC+FN_CC)\n",
    "# True Negative Rate\n",
    "TNR_CC = TN_CC/(TN_CC+FP_CC) \n",
    "# False Positive Rate\n",
    "FPR_CC = FP_CC/(FP_CC+TN_CC)\n",
    "# False Negative Rate\n",
    "FNR_CC = FN_CC/(TP_CC+FN_CC)\n",
    "\n",
    "\n",
    "# Ratio of False Positive Rates and False Negative Rates between African Americans and caucasians\n",
    "\n",
    "FPR_ratio = FPR_AA/FPR_CC \n",
    "FNR_ratio = FNR_AA/FNR_CC\n",
    "\n",
    "# Printing Results\n",
    "print(f\"The False Positive Rate for AFrical-Americans is : {FPR_AA*100:.4f} %\")\n",
    "print(f\"The False Positive Rate for Caucasians is : {FPR_CC*100:.4f} %\")\n",
    "print(f\"The False Negative Rate for AFrical-Americans is : {FNR_AA*100:.4f} %\")\n",
    "print(f\"The False Negative Rate for Caucasians is : {FNR_CC*100:.4f} %\")\n",
    "print(f\"The ratio of False Positive rates betwee African Americans and Caucasians is {FPR_ratio:.4f}\")\n",
    "print(f\"The ratio of False Negative rates betwee African Americans and Caucasians is {FNR_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Refitting the logistic model in but this time without race as a predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the ogistic refgression object\n",
    "\n",
    "lreg = LogisticRegression(random_state=66, max_iter=20000)\n",
    "\n",
    "# Redefining X for this question\n",
    "# y_new_val remains same\n",
    "\n",
    "X_new_train = X_train.drop(columns='race')\n",
    "X_new_val   = X_val.drop(columns='race')\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "\n",
    "lreg.fit(X_new_train, y_new_train)\n",
    "\n",
    "# Making prediction \n",
    "\n",
    "y_train_pred = lreg.predict(X_new_train)\n",
    "y_val_pred   = lreg.predict(X_new_val)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "\n",
    "accuracy = accuracy_score(y_new_val, y_val_pred)\n",
    "print(f\"The overall accuracy of the model is : {accuracy * 100:.4f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To report further findings we will seperate the data by race \n",
    "\n",
    "# African-American data\n",
    "X_val_AA = X_new_val[X_val['race'] == 1]\n",
    "y_val_AA = y_new_val[X_val['race'] == 1]\n",
    "y_val_pred_AA = y_val_pred[X_val['race'] == 1]\n",
    "\n",
    "# Caucasian data\n",
    "X_val_CC = X_new_val[X_val['race'] == 0]\n",
    "y_val_CC = y_new_val[X_val['race'] == 0]\n",
    "y_val_pred_CC = y_val_pred[X_val['race'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating True Negative False Negative True Positive and False Positive\n",
    "\n",
    "# For African-Americans\n",
    "AA_confusion_matrix = confusion_matrix(y_val_AA, y_val_pred_AA)\n",
    "\n",
    "TN_AA = AA_confusion_matrix[0][0]\n",
    "FN_AA = AA_confusion_matrix[1][0]\n",
    "TP_AA = AA_confusion_matrix[1][1]\n",
    "FP_AA = AA_confusion_matrix[0][1]\n",
    "\n",
    "# True Positive Rate\n",
    "TPR_AA = TP_AA/(TP_AA+FN_AA)\n",
    "# True Negative Rate\n",
    "TNR_AA = TN_AA/(TN_AA+FP_AA) \n",
    "# False Positive Rate\n",
    "FPR_AA = FP_AA/(FP_AA+TN_AA)\n",
    "# False Negative Rate\n",
    "FNR_AA = FN_AA/(TP_AA+FN_AA)\n",
    "\n",
    "\n",
    "# For Caucasians\n",
    "CC_confusion_matrix = confusion_matrix(y_val_CC, y_val_pred_CC)\n",
    "\n",
    "TN_CC = CC_confusion_matrix[0][0]\n",
    "FN_CC = CC_confusion_matrix[1][0]\n",
    "TP_CC = CC_confusion_matrix[1][1]\n",
    "FP_CC = CC_confusion_matrix[0][1]\n",
    "\n",
    "# True Positive Rate\n",
    "TPR_CC = TP_CC/(TP_CC+FN_CC)\n",
    "# True Negative Rate\n",
    "TNR_CC = TN_CC/(TN_CC+FP_CC) \n",
    "# False Positive Rate\n",
    "FPR_CC = FP_CC/(FP_CC+TN_CC)\n",
    "# False Negative Rate\n",
    "FNR_CC = FN_CC/(TP_CC+FN_CC)\n",
    "\n",
    "\n",
    "# Ratio of False Positive Rates and False Negative Rates between African Americans and caucasians\n",
    "\n",
    "FPR_ratio = FPR_AA/FPR_CC \n",
    "FNR_ratio = FNR_AA/FNR_CC\n",
    "\n",
    "# Printing Results\n",
    "print(f\"The False Positive Rate for AFrical-Americans is : {FPR_AA*100:.4f} %\")\n",
    "print(f\"The False Positive Rate for Caucasians is : {FPR_CC*100:.4f} %\")\n",
    "print(f\"The False Negative Rate for AFrical-Americans is : {FNR_AA*100:.4f} %\")\n",
    "print(f\"The False Negative Rate for Caucasians is : {FNR_CC*100:.4f} %\")\n",
    "print(f\"The ratio of False Positive rates betwee African Americans and Caucasians is {FPR_ratio:.4f}\")\n",
    "print(f\"The ratio of False Negative rates betwee African Americans and Caucasians is {FNR_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Now using logistic regression from above and plot the Receiver Operating Characteristic curve for two races, African Americans & Caucasians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC Curve\n",
    "\n",
    "# For African-American\n",
    "y_pred_proba_AA = lreg.predict_proba(X_val_AA)[::,1]\n",
    "FPR_AA, TPR_AA, threshold_AA = roc_curve(y_val_AA,  y_pred_proba_AA)\n",
    "auc_AA = roc_auc_score(y_val_AA, y_pred_proba_AA)\n",
    "\n",
    "\n",
    "\n",
    "# For Caucasian\n",
    "y_pred_proba_CC = lreg.predict_proba(X_val_CC)[::,1]\n",
    "FPR_CC, TPR_CC, threshold_CC = roc_curve(y_val_CC,  y_pred_proba_CC)\n",
    "auc_CC = roc_auc_score(y_val_CC, y_pred_proba_CC)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(FPR_AA,TPR_AA,label=f\"African-American ROC curve (area ={auc_AA:.2f})\" )\n",
    "plt.plot(FPR_CC,TPR_CC,label=f\"Caucasian ROC curve (area = {auc_CC:.2f})\" )\n",
    "plt.plot([0, 1], [0, 1], color=\"green\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for African-American and Caucasian \")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a Decision Tree model and find the best depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for depth values upto a max depth\n",
    "max_depth = 10\n",
    "depth = list(np.arange(1,max_depth + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Empyt lists to store error\n",
    "validation_error, training_error = [], []\n",
    "\n",
    "for x in depth:\n",
    "    # Initialize Classifier\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=x, random_state=x)\n",
    "\n",
    "    # Cross-Validating\n",
    "    mse_score = cross_validate(clf, X_train, y_new_train,cv=5, scoring=\"neg_mean_squared_error\")\n",
    "    validation_error.append(-1*(np.mean(mse_score['test_score'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(depth, validation_error)\n",
    "plt.xlabel(\"Max Depth of the Decision Tree\")\n",
    "plt.ylabel(\"Mean Validation Error\")\n",
    "plt.title(\"Mean Error Across Max-Depth Levels of Decision Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_mse = min(validation_error)\n",
    "best_depth_index = validation_error.index(min_val_mse)\n",
    "best_depth = depth[best_depth_index]\n",
    "print(f\"The best depth value came out to be : {best_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model performance at best depth\n",
    "clf = tree.DecisionTreeClassifier(max_depth=best_depth)\n",
    "clf.fit(X_train, y_new_train)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_new_val, y_val_pred)\n",
    "\n",
    "print(f\"The accurance of the model is : {accuracy*100:.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.tree and pydot to convert our Decision Tree to a png file\n",
    "tree.export_graphviz(clf,out_file=\"tree.dot\",filled = True)\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('Decision_Tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
